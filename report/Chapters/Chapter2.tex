% Chapter 1

\chapter{Deep Learning: Overview} % Main chapter title

\label{Chapter2} % For referencing the chapter elsewhere, use \ref{Chapter1} 

%----------------------------------------------------------------------------------------
% Define some commands to keep the formatting separated from the content 
%\newcommand{\keyword}[1]{\textbf{#1}}
%\newcommand{\tabhead}[1]{\textbf{#1}}
%\newcommand{\code}[1]{\texttt{#1}}
%\newcommand{\file}[1]{\texttt{\bfseries#1}}
%\newcommand{\option}[1]{\texttt{\itshape#1}}

In this chapter, we will provide a short overview of the theoretical concepts and recent advances in Deep Learning. We will give a basic introduction to Neural Networks, where we will discuss Convolutional Neural Networks in more detail as these are the type of algorithms utilized in this work. We further will summarize some of the most popular Convolutional Neural Network architectures.



%----------------------------------------------------------------------------------------

\section{Introduction to Deep Learning}

Deep Learning (DL) models have led to vast performance improvements in a large variety of domains, and therefore gained substantial popularity over the last decade. These models were initially inspired by the human brain and analogies in Neuroscience, which is why this class of algorithms is called Neural Networks (NN). NNs are divided into two major areas: Convolutional Neural Networks (CNN) and Recurrent Neural Networks (RNN). CNNs have driven major breakthroughs in Visual Object Recognition \parencite{krizhevsky2012}, and image \parencite{zhang2015}, video \parencite{tompson2014} and audio \parencite{hinton2012} processing while RNNs brought about advances in research on sequential data, i.e. in speech and text \parencite{collobert2011}. However, the superior performance of Neural Networks compared to traditional Machine Learning algorithms is not limited to the aforementioned domains. Other fields in which NNs have advanced the state-of-the-art include bioinformatics \parencite{junshui2015}	 and the analysis of elementary particle accelerator data ~\parencite{ciodaroc2012}.

Neural Networks usually map a fixed size input (e.g. an image) to a fixed size output (e.g. a category or a probability), where their structure is composed of multipe processing layers (see Fig.). These layers are a stack of several simple models that are able to learn arbitrarily complex non-linear input-output mappings.


Ideas from Deep Learning Review: LeCun
Big Data

Structure of Neural Networks: composed of multiple processing layers

DeepLearning --> representation based methods. Raw data --> automatic feture extraction

Mutliple levels of representation, non-linear moduls 

Although Artificial Neural Networks have been developed and studied since the 1950s, just now, due to advances in computing power, processing large datasets and backpropagation algorithm

Learning: Machine adjusts internal parameters (weights), to output correct scores. Loss function
Learning: Objective function gradient, objective function, backprop=chain rule

DL architecture: multilayer stack of simple models, non-linear input-output mappings. Represent any complex function

Structure of few layer network. Linear function + non-linear activation

Start with Figure 1 from LeCun paper to explain architecture:
- General layout
- Hidden units, weights, input output relation
- Linear mapping + non-linear Relu
- Learning: Backpropagation


\section{Convolutional Neural Networks}


\section{CNN Architectures}